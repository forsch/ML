{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AlexNet模型结构的简单介绍\n",
    "---\n",
    "### 模型组成\n",
    "\n",
    "- 输入层\n",
    "- 5个卷积层\n",
    "- 3个全链接层\n",
    "\n",
    "其中3个卷积层进行了最大池化。\n",
    "\n",
    "AlexNet模型的结构示意图如下所示\n",
    "\n",
    "![AlexNet模型的结构示意图](https://blobscdn.gitbook.com/v0/b/gitbook-28427.appspot.com/o/assets%2F-LP3nA3nT4gbZHzCMJMd%2F-LTBUkkre31IqlSmfNzh%2F-LTBUy1SIvF0LgtUOJm2%2Falexnet.jpg?alt=media&token=0d08333f-706e-4ddc-be3d-8228b0ad1f47)\n",
    "\n",
    "\n",
    "**模型主要结构：** *输入数据--->卷积、池化、ReLu激活--->卷积、池化、ReLu激活--->卷积、ReLu激活--->卷积、ReLu激活--->卷积、池化、ReLu激活--->全连接、ReLu激活、droupout--->全连接、ReLu激活、droupout--->输出*\n",
    "\n",
    "### AlexNet 个各层的详细描述\n",
    "**输入层：** 输入大小为224 x 224的3通道图像\n",
    "\n",
    "**第1层：卷积层(卷积、池化)**\n",
    "\n",
    "1. 使用96个大小为11 x 11 x 3的卷积核，分两组(每组48个)，按步长4个像素对输入层进行卷积运算，得到两组55 x 55 x48 的卷积结果。\n",
    "\n",
    "2. 对卷积结果使用ReLu激活函数，得到激活结果。\n",
    "\n",
    "3. 对激活结果使用窗口为3 x 3、步长为2个像素的重叠最大池化，得到27 x 27 x 48的池化结果。\n",
    "\n",
    "4. 对池化结果使用局部响应归一化操作，得到27 x 27 x 48的归一化结果。\n",
    "\n",
    "**第2层：卷积层(卷积、池化)**\n",
    "\n",
    "1. 使用256个大小为27 x 27 x 48的卷积核，分两组(每组128个)，按步长1个像素对第2层的归一化结果进行卷积运算，得到两组27 x 27 x 128 的卷积结果。\n",
    "\n",
    "2. 对卷积结果使用ReLu激活函数，得到激活结果。\n",
    "\n",
    "3. 对两组 27 x 27 x 128的激活结果使用窗口为3 x 3、步长为2个像素的重叠最大池化，得到两组13 x 13 x 128的池化结果。 \n",
    "\n",
    "4. 对池化结果使用局部响应归一化操作，得到两组13 x 13 x128 的归一化结果。\n",
    "\n",
    "**第3层：卷积层**\n",
    "\n",
    "1. 使用384个大小为13 x 13 x 256的卷集核，分两组，按步长为1个像素对上一层归一化结果卷积运算，得到两组13 x 13 x 192的卷积结果。\n",
    "\n",
    "2. 对卷积结果使用ReLu激活函数，得到激活结果。\n",
    "\n",
    "**第4层：卷积层**\n",
    "\n",
    "1. 使用384个大小为13 x 13 x 192的卷集核，分两组，按步长为1个像素对上一层的激活结果进行卷积运算，得到两组两组 13 x 13 x 192 的卷积结果。\n",
    "\n",
    "2. 对卷积结果使用ReLu激活函数，得到激活结果\n",
    "\n",
    "**第5层：卷积层(卷积、池化)**\n",
    "\n",
    "1. 使用256个大小为13 x 13 x 192的卷积核分两组，按步长为1个像素，对上一层的激活结果卷积运算，得到两组 13 x 13 x128卷积结果。\n",
    "\n",
    "2. 对卷积结果使用ReLu激活函数，得到激活结果。\n",
    "\n",
    "3. 对激活结果，进行窗口为3 x 3,步长为2个像素的重叠最大池化，得到两组13 x 3 x 128的池化结果。\n",
    "\n",
    "**第6层： 全链接层**\n",
    "\n",
    "1. 使用4096个神经元分两组，对上一层的池化结果全链接处理。\n",
    "\n",
    "2. 对全链接结果使用ReLu激活函数。\n",
    "\n",
    "3. 对激活结果使用概率为0.5的dropout操作，得到dropout结果。\n",
    "\n",
    "**第7层： 全链接层**\n",
    "\n",
    "1. 使用4096个神经元分两组，对上一层的池化结果全链接处理。\n",
    "\n",
    "2. 对全链接结果使用ReLu激活函数。\n",
    "\n",
    "3. 对激活结果使用概率为0.5的dropout操作，得到dropout结果。\n",
    "\n",
    "**第8层： 输出层**\n",
    "\n",
    "1. 1000路的软最大输出层，用来覆盖1000类的标签分布。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
